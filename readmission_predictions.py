# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YMW4YwRjCxngxK2-ZOmqHi_JLHYUEBqx
"""

# ‚úÖ FINAL COLAB-READY RANDOM FOREST MODEL WITH FIXED AGE HANDLING

# üì¶ INSTALL REQUIRED LIBRARIES
!pip install imbalanced-learn scikit-learn pandas matplotlib seaborn

# üìö IMPORTS
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from imblearn.over_sampling import SMOTE


# üì• LOAD DATA
patients = pd.read_csv("patients_cleaned.csv")
conditions = pd.read_csv("conditions_cleaned.csv")
encounters = pd.read_csv("encounters_cleaned.csv")
observations = pd.read_csv("observations_cleaned.csv")
procedures = pd.read_csv("procedures_cleaned.csv")

# üìå PARSE DATES
patients['BIRTHDATE'] = pd.to_datetime(patients['BIRTHDATE'])
conditions['START'] = pd.to_datetime(conditions['START'])
encounters['START'] = pd.to_datetime(encounters['START'])
encounters['STOP'] = pd.to_datetime(encounters['STOP'])
observations['DATE'] = pd.to_datetime(observations['DATE'])
procedures['START'] = pd.to_datetime(procedures['START'])

# üè∑Ô∏è READMISSION LABELING
encounters = encounters.sort_values(['PATIENT', 'START']) # Changed 'patient' to 'PATIENT'
encounters['days_to_next'] = encounters.groupby('PATIENT')['START'].diff().shift(-1).dt.days # Changed 'patient' to 'PATIENT'
encounters['unplanned_readmit'] = (
    (encounters['days_to_next'] <= 30) &
    (encounters['ENCOUNTERCLASS'].shift(-1) == 'emergency')
)
y = encounters.groupby('PATIENT')['unplanned_readmit'].sum() # Changed 'patient' to 'PATIENT'
y_binary = (y > 0).astype(int)

# üìä CALCULATE AGE
encounters = encounters.merge(patients, left_on='PATIENT', right_on='Id', how='left')
encounters['BIRTHDATE'] = pd.to_datetime(encounters['BIRTHDATE'])
# Convert 'START' to tz-naive before subtraction if it is tz-aware
encounters['AGE'] = (encounters['START'].dt.tz_localize(None) - encounters['BIRTHDATE']).dt.days // 365
# üß† CHRONIC CONDITIONS
chronic_conditions = {
    'Diabetes': ['diabet', 'hyperglycem', 'dm type'],
    'Heart_Failure': ['heart fail', 'chf', 'cardiac insuff'],
    'COPD': ['copd', 'chronic obstruct', 'emphysema'],
    'CKD': ['chronic kidney', 'renal insuff', 'ckd'],
    'Hypertension': ['hypertens', 'htn', 'high blood pressure'],
    'Obesity': ['obes', 'bmi [>=]40', 'morbidly obese'],
    'Anemia': ['anem', 'low hemoglobin', 'iron deficien'],
    'Prediabetes': ['prediabet', 'impaired glucose'],
    'Coronary_Artery_Disease': ['cad', 'coronary artery'],
    'Asthma': ['asthma', 'reactive airway']
}
conditions['chronic_type'] = None
for condition, keywords in chronic_conditions.items():
    mask = conditions['DESCRIPTION'].str.contains('|'.join(keywords), case=False, na=False)
    conditions.loc[mask, 'chronic_type'] = condition

chronic_flags = pd.get_dummies(
    conditions[conditions['chronic_type'].notna()]
    .groupby('PATIENT')['chronic_type']
    .value_counts()
    .unstack(fill_value=0),
    prefix='has'
)

# üîß FEATURE ENGINEERING
total_conditions = conditions.groupby('PATIENT').agg(
    total_conditions=('CODE', 'nunique'),
    recent_condition=('START', lambda x: (pd.Timestamp.now() - x.max().tz_localize(None)).days)
)

encounter_features = encounters.groupby('PATIENT').agg(
    total_visits=('Id_x', 'count'),
    er_visits=('ENCOUNTERCLASS', lambda x: (x == 'emergency').sum()),
    avg_visit_cost=('BASE_ENCOUNTER_COST', 'mean'),
    last_visit_days=('START', lambda x: (pd.Timestamp.now() - x.max().tz_localize(None)).days),
    AGE=('AGE', 'mean')
)

observations['is_abnormal'] = observations['DESCRIPTION'].str.contains('high|low|abnormal|elevated', case=False, na=False)
observation_features = observations.groupby('PATIENT').agg(
    abnormal_labs=('is_abnormal', 'sum'),
    unique_tests=('CODE', 'nunique')
)

procedure_features = procedures.groupby('PATIENT').agg(
    total_procedures=('PATIENT', 'count'),
    surgical=('DESCRIPTION', lambda x: x.str.contains('surg', case=False).sum()),
    avg_procedure_cost=('BASE_COST', 'mean')
)

# üîó MERGE FEATURES
X = patients[['Id', 'GENDER', 'RACE']].rename(columns={'Id': 'PATIENT'})
X = pd.get_dummies(X, columns=['GENDER', 'RACE'], drop_first=True)
X = (X
     .merge(encounter_features, on='PATIENT', how='left')
     .merge(chronic_flags, on='PATIENT', how='left')
     .merge(total_conditions, on='PATIENT', how='left')
     .merge(observation_features, on='PATIENT', how='left')
     .merge(procedure_features, on='PATIENT', how='left')
     .fillna(0)
     .set_index('PATIENT'))

# üìè SCALE
scaler = StandardScaler()
X_scaled = pd.DataFrame(scaler.fit_transform(X), index=X.index, columns=X.columns)

# ‚öñÔ∏è BALANCE WITH SMOTE
# Ensure y_binary has the same index as X_scaled and drop NaNs if any
y_binary = y_binary.reindex(X_scaled.index).dropna()

# Select only the rows in X_scaled that have corresponding entries in y_binary
X_scaled = X_scaled.loc[y_binary.index]

smote = SMOTE(random_state=42)
X_balanced, y_balanced = smote.fit_resample(X_scaled, y_binary)

# üéØ GRIDSEARCHCV
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
    'class_weight': ['balanced']
}
grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid,
                           cv=3, scoring='roc_auc', n_jobs=-1, verbose=1)
grid_search.fit(X_balanced, y_balanced)
best_model = grid_search.best_estimator_

# üß† TRAIN & EVALUATE
X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)
best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_test)
y_pred_proba = best_model.predict_proba(X_test)[:, 1]

print("\nClassification Report:\n")
print(classification_report(y_test, y_pred))
print(f"AUC-ROC: {roc_auc_score(y_test, y_pred_proba):.2f}")

# üîç FEATURE IMPORTANCE
importances = pd.Series(best_model.feature_importances_, index=X.columns)
print("\nTop 10 Features:\n")
print(importances.sort_values(ascending=False).head(10))

importances.sort_values().plot(kind='barh', figsize=(10, 6), title='Feature Importance')
plt.xlabel("Importance")
plt.tight_layout()
plt.show()

# üîé CONFUSION MATRIX
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=[0, 1], yticklabels=[0, 1])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix After SMOTE")
plt.tight_layout()
plt.show()

# ‚úÖ GENERATE FINAL PREDICTIONS ON X_balanced
final_predictions = best_model.predict(X_balanced)

# Recreate patient index from the balanced data
balanced_patients = pd.DataFrame(X_balanced, columns=X.columns)

# Create the 'PATIENT' column based on the original y_binary index and the balancing technique
# Assuming SMOTE duplicates minority class samples, we need to repeat the index accordingly
# First get the unique patients and their counts in the original y_binary
patient_counts = y_binary.value_counts()

# Then create a list of patient IDs repeated by their original counts plus SMOTE-added samples
repeated_patients = []
for patient, count in patient_counts.items():
    # If patient is in minority class (count < majority class count), repeat extra times
    if count < patient_counts.max():  # Assuming binary classification
        repeat_times = patient_counts.max()  # Balance by repeating to majority class count
    else:
        repeat_times = count  # Keep majority class count as is

    repeated_patients.extend([patient] * repeat_times)

balanced_patients["PATIENT"] = repeated_patients

# Combine with predictions
balanced_patients["PredictedReadmission"] = final_predictions

# Select and export
output_df = balanced_patients[["PATIENT", "PredictedReadmission"]]
output_df.to_csv("readmission_predictions_binary.csv", index=False)

# üì• Download
from google.colab import files
files.download("readmission_predictions_binary.csv")

!pip install imbalanced-learn

# ‚úÖ GENERATE FINAL PREDICTIONS
predictions = best_model.predict(X_balanced)

# Create dummy PATIENT IDs if lost during SMOTE
output_df = pd.DataFrame(X_balanced, columns=X.columns)
output_df["PATIENT"] = ["P" + str(i) for i in range(len(output_df))]
output_df["PredictedReadmission"] = predictions

# Export CSV
output_df[["PATIENT", "PredictedReadmission"]].to_csv("readmission_predictions_binary.csv", index=False)

# Download
from google.colab import files
files.download("readmission_predictions_binary.csv")